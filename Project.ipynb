{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fbf49e-f6a3-4be2-a21f-087946ac0595",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get the file name from the adf\n",
    "fileName = dbutils.widgets.get('fileName') # once we have connected Azure Data factory \n",
    "#fileName='Product.csv'\n",
    "fileNameWithoutExt = fileName.split('.')[0] # changing file name to product from product.csv as we saved value as Product in db\n",
    "print(fileNameWithoutExt)\n",
    "print(dbutils.secrets.listScopes()) # list all scopes in databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6259a115-ab8d-46f2-9a36-7bda72962b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#from datetime import datetme as dt\n",
    "\n",
    "#Just change all the values here based on the resource name you have created in your environemnt and workspace.\n",
    "\n",
    "sqlDbName = 'db ' # db name\n",
    "dbUserName = 'project1' # username of db\n",
    "passwordKey = '' # secret scored in db for password\n",
    "stgAccountSASTokenKey = ''# sas token stored in key vault\n",
    "landingFileName =fileName #'Product'  #dbutils.widgets.get('Product')\n",
    "databricksScopeName ='project1scope'\n",
    "dbServer = 'project1databaseserver'\n",
    "dbServerPortNumber ='1433' # bydefault for azure sql server\n",
    "storageContainer ='inputdata'\n",
    "storageAccount='' # adls storage name\n",
    "landingMountPoint ='/mnt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346bbbec-6feb-4c94-9944-282a83bbe72f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.get(databricksScopeName,stgAccountSASTokenKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1791d71e-246d-4fb9-ab59-9283fc2a0267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not any(mount.mountPoint == landingMountPoint for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount( source = 'wasbs://{}@{}.blob.core.windows.net'.format(storageContainer, storageAccount), mount_point= landingMountPoint, extra_configs ={'fs.azure.sas.{}.{}.blob.core.windows.net'.format(storageContainer,storageAccount):dbutils.secrets.get(scope = databricksScopeName, key= stgAccountSASTokenKey)})\n",
    "    print('Mounted the storage account successfully')\n",
    "else:\n",
    "    print('Storage account already mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be56aba3-aee9-4567-8590-49cbf7dc6eb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connect to Azure SQL DB\n",
    "dbPassword = dbutils.secrets.get(scope = databricksScopeName, key= passwordKey)\n",
    "serverurl = 'jdbc:sqlserver://{}.database.windows.net:{};database={};user={};'.format(dbServer, dbServerPortNumber,sqlDbName,dbUserName)\n",
    "connectionProperties = {\n",
    "    'password':dbPassword,\n",
    "    'driver':'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "}\n",
    "df = spark.read.jdbc(url = serverurl, table = 'dbo.FileDetailsFormat', properties= connectionProperties)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984b5a92-1256-4fa6-ae7f-80cbf7ac7693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('/mnt/landing/'+fileName, inferSchema=True, header=True)\n",
    "#display(df1)\n",
    "df2 = df.filter(df.FileName==fileNameWithoutExt).select('ColumnName','ColumnDateFormat' )\n",
    "rows = df2.collect()\n",
    "for r in rows:\n",
    "    colName = r[0]\n",
    "    colFormat =r[1]\n",
    "    print(colName, colFormat)\n",
    "    #display(df1.filter(F.to_date(colName, colFormat).isNull() ==True))\n",
    "    formatCount =df1.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n",
    "print(formatCount)\n",
    "totalcount = df1.count()\n",
    "print(totalcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638c358e-b149-4675-ad1c-33376b3d542e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('/mnt/landing/'+fileName, inferSchema=True, header=True)\n",
    "display(df1)\n",
    "\n",
    "# Rule\n",
    "errorFlag=False\n",
    "errorMessage = ''\n",
    "totalcount = df1.count()\n",
    "print(totalcount)\n",
    "distinctCount = df1.distinct().count()\n",
    "print(distinctCount)\n",
    "if distinctCount !=totalcount:\n",
    "    errorFlag = True\n",
    "    errorMessage = 'Duplication Found. Rule 1 Failed'\n",
    "print(errorMessage)\n",
    "    \n",
    "# Rule 2\n",
    "df2 = df.filter(df.FileName==fileNameWithoutExt).select('ColumnName','ColumnDateFormat' )\n",
    "rows = df2.collect()\n",
    "for r in rows:\n",
    "    colName = r[0]\n",
    "    colFormat =r[1]\n",
    "    print(colName, colFormat)\n",
    "    #display(df1.filter(F.to_date(colName, colFormat).isNull() ==True))\n",
    "    formatCount =df1.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n",
    "    if formatCount != totalcount:\n",
    "        errorFlag = True\n",
    "        errorMessage = errorMessage +' DateFormate is incorrect for {} '.format(colName)\n",
    "    else:\n",
    "        print('All rows are good for ', colName)\n",
    "print(errorMessage)\n",
    "\n",
    "if errorFlag:\n",
    "    dbutils.fs.mv('/mnt/landing/'+fileName,'/mnt/rejected/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"true\", \"errorMessage\":\"'+errorMessage +'\"}')\n",
    "else:\n",
    "    dbutils.fs.mv('/mnt/landing/'+fileName,'/mnt/staging/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"false\", \"errorMessage\":\"No error\"}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
